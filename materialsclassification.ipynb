{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinyoungkim0214/portfolio/blob/main/materialsclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importí•˜ê¸°"
      ],
      "metadata": {
        "id": "AppKwEu4sVcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apache 2.0 - ì½”ë“œì—ì„œ ì–´ë–¤ ë¶€ë¶„ ìˆ˜ì •í–ˆëŠ”ì§€ ëª…ì‹œí•´ì•¼ í•¨"
      ],
      "metadata": {
        "id": "0J3bDHSZ4eCu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9VW25cvvre8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "625bb309-1475-4c5a-96d9-124bee7b0407"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b6ced5cbe1ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ì´ë¯¸ì§€ class 47ê°œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import os.path\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from google.colab import drive  # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ìš©\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ì´ë¯¸ì§€ class 47ê°œ\n",
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_classes(dir):\n",
        "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "def make_dataset(txtnames, datadir, class_to_idx): #ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ì´ë¯¸ì§€ ê²½ë¡œë‘ label\n",
        "    images = []\n",
        "    labels = []\n",
        "    for txtname in txtnames:\n",
        "        with open(txtname, 'r') as lines:\n",
        "            for line in lines:\n",
        "                classname = line.split('/')[0]\n",
        "                _img = os.path.join(datadir, 'images', line.strip())\n",
        "                assert os.path.isfile(_img)\n",
        "                images.append(_img)\n",
        "                labels.append(class_to_idx[classname])\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "class DTDDataloader(data.Dataset): #ì´ë¯¸ì§€ë‘ labelì´ ê°ê° ì €ì¥ì´ ë¨ #ê²½ë¡œë‘ labelì„ ì´ë¯¸ì§€ë‘ labelì— ì €ì¥ (ìš©ëŸ‰ì„ ì¤„ì´ê³  ì‹¶ì–´ì„œ, ì´ë¯¸ì§€ëŠ” ê²½ë¡œë§Œ, labelì€ í…ìŠ¤íŠ¸ë‹ˆê¹Œ ê·¸ëŒ€ë¡œ í•´ë„ ê´œì°®ë‹¤)\n",
        "    def __init__(self, DATASET_PATH, split, transform=None, train=True):\n",
        "        classes, class_to_idx = find_classes(os.path.join(DATASET_PATH, 'images'))\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "\n",
        "        if train:\n",
        "            filename = [os.path.join(DATASET_PATH, 'labels/train' + split + '.txt'),\n",
        "                        os.path.join(DATASET_PATH, 'labels/val' + split + '.txt')]\n",
        "        else:\n",
        "            filename = [os.path.join(DATASET_PATH, 'labels/test' + split + '.txt')]\n",
        "\n",
        "        self.images, self.labels = make_dataset(filename, DATASET_PATH, class_to_idx)\n",
        "        assert (len(self.images) == len(self.labels))\n",
        "\n",
        "    def __getitem__(self, index):#ì´ë¯¸ì§€ì˜ ì¸ë±ìŠ¤ê°€ 0~ ì´ë¯¸ì§€ ê²½ë¡œ ì˜¤í”ˆí•´ì„œ rgbë¡œ ë³€í™˜\n",
        "        _img = Image.open(self.images[index]).convert('RGB') #ì—¬ê¸°ì—rgbì €ì¥\n",
        "        _label = self.labels[index]\n",
        "        if self.transform is not None:\n",
        "            _img = self.transform(_img)\n",
        "\n",
        "        return _img, _label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "class Dataloder():\n",
        "    def __init__(self, DATASET_PATH, split, batch_size):\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225]) #ì •ê·œí™”\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            #transforms.Resize(256),\n",
        "            #transforms.CenterCrop(224),\n",
        "            transforms.Resize(64),\n",
        "            #transforms.CenterCrop(32),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        trainset = DTDDataloader(DATASET_PATH, split, transform_train, train=True)\n",
        "        testset = DTDDataloader(DATASET_PATH, split, transform_test, train=False)\n",
        "\n",
        "        kwargs = {'num_workers': 8, 'pin_memory': True}\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=\n",
        "        batch_size, shuffle=True, **kwargs)\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=\n",
        "        batch_size, shuffle=False, **kwargs)\n",
        "        self.classes = trainset.classes\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "\n",
        "    def getloader(self): #tensorë¡œ ë°”ê¿”ì£¼ê³  í•™ìŠµì‹œí‚¤ê¸°\n",
        "        return self.classes, self.trainloader, self.testloader"
      ],
      "metadata": {
        "id": "1vX7P2Vhm-Wf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì „ì²˜ë¦¬(pre-processing)\n",
        "* í•™ìŠµ data, train, validation, testë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆŒ ê±´ì§€ (ë°ì´í„° 6:2:2ë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆŒ ê±´ì§€?) - ì´ ìˆ˜ì²œ~ìˆ˜ë§Œ ì¥ì€ í•„ìš”í•¨"
      ],
      "metadata": {
        "id": "s9F7mf3TsX1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "AKil77E6sXYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "!unzip \"dataset.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDUuDST1rcNH",
        "outputId": "19351af6-21f1-422b-ffd7-734772177fc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "unzip:  cannot find or open dataset.zip, dataset.zip.zip or dataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë‚´ dataset/dtd í´ë”)\n",
        "DATASET_PATH = '/content/drive/MyDrive/dataset/dtd'\n",
        "split = '1'\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "8NO3TP91pmm5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì‹œ: ë°°ì¹˜ ì‚¬ì´ì¦ˆ 32, split=1ë¡œ ë°ì´í„°ë¡œë” ìƒì„±\n",
        "dataloader = Dataloder(DATASET_PATH, batch_size=32, split='1')\n",
        "classes, trainloader, testloader = dataloader.getloader()\n",
        "\n",
        "# ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸\n",
        "for images, labels in trainloader:\n",
        "    print(images.shape, labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "37YvoH1wm-58",
        "outputId": "b22306d1-16e4-4870-d117-4f62b00937a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/dtd/images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a90e50fadd23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ì˜ˆì‹œ: ë°°ì¹˜ ì‚¬ì´ì¦ˆ 32, split=1ë¡œ ë°ì´í„°ë¡œë” ìƒì„±\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-33dd28389dd7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, DATASET_PATH, split, batch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m         ])\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDTDDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDTDDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-33dd28389dd7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, DATASET_PATH, split, transform, train)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDTDDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#ì´ë¯¸ì§€ë‘ labelì´ ê°ê° ì €ì¥ì´ ë¨ #ê²½ë¡œë‘ labelì„ ì´ë¯¸ì§€ë‘ labelì— ì €ì¥ (ìš©ëŸ‰ì„ ì¤„ì´ê³  ì‹¶ì–´ì„œ, ì´ë¯¸ì§€ëŠ” ê²½ë¡œë§Œ, labelì€ í…ìŠ¤íŠ¸ë‹ˆê¹Œ ê·¸ëŒ€ë¡œ í•´ë„ ê´œì°®ë‹¤)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-33dd28389dd7>\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/dtd/images'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "network ì„¤ê³„\n",
        "* ì–´ë–¤ networkë¥¼ ì‚¬ìš©í•  ê±´ì§€\n",
        "* ì°¨ì› ê³„ì‚°"
      ],
      "metadata": {
        "id": "G_7QY8JbsYFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes: int, out_planes: int, stride: int = 1, downsample: nn.Module = None,\n",
        "                 padding_mode: str = 'constant'):\n",
        "        super().__init__()\n",
        "        self.conv1 = self._make_conv(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                                     padding=1, padding_mode=padding_mode)\n",
        "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
        "        self.conv2 = self._make_conv(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                                     padding=1, padding_mode=padding_mode)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def _make_conv(self, in_ch, out_ch, kernel_size, stride, padding, padding_mode):\n",
        "        conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=stride,\n",
        "                         padding=padding, bias=False)\n",
        "        return conv\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module): #convì—¬ëŸ¬ê°œ -> bottleneck í•œ ë©ì–´ë¦¬ -> ì—¬ëŸ¬ê°œ -> Resnet\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes: int, out_planes: int, stride: int = 1, downsample: nn.Module = None,\n",
        "                 padding_mode: str = 'constant'):\n",
        "        super().__init__()\n",
        "        mid_planes = out_planes\n",
        "        self.conv1 = self._make_conv(in_planes, mid_planes, kernel_size=1, stride=1,\n",
        "                                     padding=0, padding_mode=padding_mode)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
        "        self.conv2 = self._make_conv(mid_planes, mid_planes, kernel_size=3, stride=stride,\n",
        "                                     padding=1, padding_mode=padding_mode)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_planes)\n",
        "        self.conv3 = self._make_conv(mid_planes, out_planes * self.expansion, kernel_size=1, stride=1,\n",
        "                                     padding=0, padding_mode=padding_mode)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def _make_conv(self, in_ch, out_ch, kernel_size, stride, padding, padding_mode):\n",
        "        conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=stride,\n",
        "                         padding=padding, bias=False)\n",
        "        return conv\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "resnet_cfgs: Dict[str, Tuple[Union[BasicBlock, Bottleneck], List[int]]] = {\n",
        "    \"resnet18\":  (BasicBlock,  [2, 2, 2, 2]), #bottleneckë³´ë‹¤ ê°€ë²¼ìš´ê²Œ basicblock\n",
        "    \"resnet34\":  (BasicBlock,  [3, 4, 6, 3]), #(3+4+6+3)*2+1+1 (ë§¨ ë§ˆì§€ë§‰ ë¶€ë¶„ 2ê°œ)\n",
        "\n",
        "    \"resnet50\":  (Bottleneck, [3, 4, 6, 3]), #ì—¬ê¸°ì„œëŠ” 34ë¡œ ëŒë¦¼\n",
        "    \"resnet101\":  (Bottleneck, [3, 4, 23, 3]), #(3+4+23+3)*3+1+1=101\n",
        "    \"resnet152\":  (Bottleneck, [3, 8, 36, 3]) #(3+8+36+3)*3+1+1=152\n",
        "    }\n",
        "\n",
        "\n",
        "def make_resnet_layers(\n",
        "    resnet_type: str = \"resnet18\",\n",
        "    in_channels: int = 1,\n",
        "    padding_mode: str = \"constant\"\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Modified to return a module that can output intermediate layers.\n",
        "    \"\"\"\n",
        "    block_class, layer_cfg = resnet_cfgs[resnet_type]\n",
        "\n",
        "    class ResNetLayers(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            # Initial conv7x7 + BN + ReLU + MaxPool\n",
        "            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            # Define layers\n",
        "            self.layer1 = self._make_layer(block_class, 64, 64, layer_cfg[0], stride=1, padding_mode=padding_mode)\n",
        "            self.layer2 = self._make_layer(block_class, 64 * block_class.expansion, 128, layer_cfg[1], stride=2, padding_mode=padding_mode)\n",
        "            self.layer3 = self._make_layer(block_class, 128 * block_class.expansion, 256, layer_cfg[2], stride=2, padding_mode=padding_mode)\n",
        "            self.layer4 = self._make_layer(block_class, 256 * block_class.expansion, 512, layer_cfg[3], stride=2, padding_mode=padding_mode)\n",
        "\n",
        "        def _make_layer(self, block, in_ch, out_ch, num_blocks, stride, padding_mode):\n",
        "            downsample = None\n",
        "            if stride != 1 or in_ch != out_ch * block.expansion:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(in_ch, out_ch * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(out_ch * block.expansion),\n",
        "                )\n",
        "            blocks = []\n",
        "            blocks.append(block(in_ch, out_ch, stride=stride, downsample=downsample, padding_mode=padding_mode))\n",
        "            in_ch = out_ch * block.expansion\n",
        "            for _ in range(1, num_blocks):\n",
        "                blocks.append(block(in_ch, out_ch, stride=1, downsample=None, padding_mode=padding_mode))\n",
        "            return nn.Sequential(*blocks)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.maxpool(x)\n",
        "            out1 = self.layer1(x)\n",
        "            out2 = self.layer2(out1)\n",
        "            out3 = self.layer3(out2)\n",
        "            out4 = self.layer4(out3)\n",
        "            return out4\n",
        "\n",
        "    return ResNetLayers()\n",
        "\n",
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified to return outputs from layer1, layer2, layer3, and layer4.\n",
        "    Example:\n",
        "        model = ResNetFeatureExtractor(resnet_type='resnet18', in_channels=1)\n",
        "        outs = model(x)  # x: (B,1,192,96), outs: list of [B,64,96,48], [B,128,48,24], [B,256,24,12], [B,512,12,6]\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        resnet_type: str = \"resnet18\",\n",
        "        in_channels: int = 3,\n",
        "        padding_mode: str = 'constant'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert resnet_type in resnet_cfgs, f\"Unsupported ResNet type: {resnet_type}\"\n",
        "\n",
        "        self.resnet_type = resnet_type\n",
        "        self.in_channels = in_channels\n",
        "        self.padding_mode = padding_mode\n",
        "        self.feature_extractor = make_resnet_layers(resnet_type, in_channels, padding_mode)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ],
      "metadata": {
        "id": "ZtToES5Xlxjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.fe = ResNetFeatureExtractor(resnet_type=\"resnet152\", in_channels=3) #resnetfeatureextractor: backboneë¶€ë¶„\n",
        "        self.fe = ResNetFeatureExtractor(resnet_type=\"resnet101\", in_channels=3)\n",
        "        #self.fe = ResNetFeatureExtractor(resnet_type=\"resnet50\", in_channels=3)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, 47) #ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •í•˜ê¸° (í´ë˜ìŠ¤ 47ê°œ)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fe(x)\n",
        "        #ì—¬ê¸°ì„œë¶€í„° head\n",
        "        x = self.avgpool(x)\n",
        "        print('avgpool shape:', x.shape)\n",
        "        x = torch.flatten(x, 1)\n",
        "        print('flatten shape:', x.shape)\n",
        "        x = self.fc(x)\n",
        "        print('fc shape:', x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "net=BaselineModel1()"
      ],
      "metadata": {
        "id": "jKjZBEOYzBWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameter ì •ì˜\n",
        "* ì–´ë–¤ lossë¥¼ ì“¸ ê±´ì§€\n",
        "* ì–´ë–¤ optimizerë¥¼ ì“¸ ê±´ì§€\n",
        "* í•™ìŠµë¥ ì€ ëª‡ìœ¼ë¡œ í•  ê±´ì§€\n",
        "* ëª‡ epochì„ í•™ìŠµì‹œí‚¬ ê±´ì§€"
      ],
      "metadata": {
        "id": "ntyRAClusYSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)  # Learning rate: í•™ìŠµë¥ . í•œ ë²ˆì˜ optimizer stepì—ì„œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ê°ˆì§€."
      ],
      "metadata": {
        "id": "OUVepM7Xzdau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "      \"\"\"Train Config\"\"\"\n",
        "      C.lr = 1e-2\n",
        "      C.lr_decay = 40\n",
        "      C.momentum = 0.9\n",
        "      C.weight_decay = 1e-4\n",
        "      C.batch_size = 64\n",
        "      C.start_epoch = 1\n",
        "      C.nepochs = 100\n",
        "      C.eval = False\n",
        "      C.split = '1'\n",
        "\n",
        "      C.cuda = True\n",
        "      C.gpu = '0'\n",
        "      C.resume = False\n",
        "      C.momentum = 0.9\n",
        "      C.weight_decay = 1e-4"
      ],
      "metadata": {
        "id": "oAqazShLtLxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train(í•™ìŠµ)"
      ],
      "metadata": {
        "id": "BonNJffXsjpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss_history = []\n",
        "test_loss_history = []"
      ],
      "metadata": {
        "id": "1IX0i_fZzicP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloder():\n",
        "    def __init__(self, DATASET_PATH, split, batch_size):\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        # --- í•™ìŠµìš©(ì¦ê°• ê·¸ëŒ€ë¡œ) ---\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        # --- ğŸ”´ ì—¬ê¸° ìˆ˜ì •: í…ŒìŠ¤íŠ¸Â·ê²€ì¦ìš©ë„ 224Ã—224ë¡œ ê³ ì • ğŸ”´ ---\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.Resize(256),       # ê¸´ ë³€ ê¸°ì¤€ 256\n",
        "            transforms.CenterCrop(224),   # ê°€ìš´ë°ì„œ 224Ã—224 ì˜ë¼ë‚´ê¸°\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "        # -------------------------------------------------------\n",
        "\n",
        "        trainset = DTDDataloader(DATASET_PATH, split, transform_train, train=True)\n",
        "        testset  = DTDDataloader(DATASET_PATH, split, transform_test,  train=False)\n",
        "\n",
        "        kwargs = {'num_workers': 8, 'pin_memory': True}\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                  shuffle=True,  **kwargs)\n",
        "        testloader  = torch.utils.data.DataLoader(testset,  batch_size=batch_size,\n",
        "                                                  shuffle=False, **kwargs)\n",
        "\n",
        "        self.classes     = trainset.classes\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader  = testloader\n"
      ],
      "metadata": {
        "id": "S_bmY0vMtzvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí•  ê±´ì§€\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # trainloaderë¡œë¶€í„° ë°ì´í„°ì™€ ë¼ë²¨ì„ ë°›ì•„ì˜µë‹ˆë‹¤.\n",
        "        inputs, labels = data\n",
        "\n",
        "        # ë§¤ ë°˜ë³µë§ˆë‹¤ ì´ì „ gradientë¥¼ í•œ ë²ˆ ì§€ì›Œì¤ë‹ˆë‹¤.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ëª¨ë¸ì— ë°ì´í„° ë„£ì–´ì„œ forward í•´ì£¼ê³ \n",
        "        # backpropìœ¼ë¡œ ì´ë²ˆ inputì— ëŒ€í•´ gradientë¥¼ ê³„ì‚°í•´ì£¼ê³ \n",
        "        # optimizerê°€ gradient descent 1ìŠ¤í… ì§„í–‰\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ê²°ê³¼ì¹˜ í™”ë©´ì— ë¿Œë ¤ì£¼ê¸°\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # 200 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥ i=0~199: 200ê°œê°€ í†µê³¼; 200ê°œë§ˆë‹¤ í”„ë¦°íŠ¸í•´ì„œ ë³´ì #ì—­ì „íŒŒí•˜ê³  ìˆìœ¼ë‹ˆê¹Œ í•™ìŠµë°ì´í„°ì— ëŒ€í•œ lossëŠ” ê³„ì† ì¤„ì–´ë“¦\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "\n",
        "            # ë‚˜ì¤‘ì— ì‹œê°í™”ë¥¼ ìœ„í•´ ì¤‘ê°„ì¤‘ê°„ ë”°ë¡œ lossê°’ ì €ì¥\n",
        "            training_loss_history.append(running_loss / 200)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                running_test_loss = 0.0\n",
        "                for i, test_data in enumerate(testloader, 0):\n",
        "                    test_images, test_labels = test_data\n",
        "                    test_outputs = net(test_images)\n",
        "                    test_loss = criterion(test_outputs, test_labels)\n",
        "                    running_test_loss += test_loss.item()\n",
        "\n",
        "                test_loss_history.append(running_test_loss / i)\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('í•™ìŠµ ë!')"
      ],
      "metadata": {
        "id": "uB57M4D5zmWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6658df6d-07b4-4741-da1a-f127787eb88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avgpool shape: torch.Size([32, 2048, 1, 1])\n",
            "flatten shape: torch.Size([32, 2048])\n",
            "fc shape: torch.Size([32, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "gwIfhucMsj38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(training_loss_history, label=\"Training Loss\")\n",
        "plt.plot(test_loss_history, 'r', label=\"Test Loss\")\n",
        "plt.title('Training & Test Loss', fontsize=20)\n",
        "plt.xlabel('Iteration',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPvYv-zBsj_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './dtd_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "VlN-U72ywV3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes, train_loader, test_loader = Dataloder(DATASET_PATH, batch_size, split).getloader()\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "metadata": {
        "id": "BUiK6iT3wWpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "qzs0MhQIytFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(images)\n",
        "outputs"
      ],
      "metadata": {
        "id": "VJYXQymhyuz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, predicted = torch.max(outputs, 1)  # 1ë²ˆì§¸ ì°¨ì›(=ê° row)ì—ì„œ ê°ê° maxì¸ ê°’ê³¼ í•´ë‹¹ indexë¥¼ ë½‘ì•„ì˜µë‹ˆë‹¤.\n",
        "\n",
        "print('ëª¨ë¸ ì˜ˆì¸¡: ', ', '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "metadata": {
        "id": "1x4uXoRNyzJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì›ë˜ ì´ë¯¸ì§€ë‘ ê°™ì´ ë³¼ê¹Œìš”?\n",
        "imshow(torchvision.utils.make_grid(images[:4]))\n",
        "print('ì‹¤ì œ ì •ë‹µ: ', ', '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "print('ëª¨ë¸ ì˜ˆì¸¡: ', ', '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "metadata": {
        "id": "bQKiXZehy1fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('10000ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì •ë‹µë¥ : %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "id": "B79vZ5Auy60d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(47))\n",
        "class_total = list(0. for i in range(47))\n",
        "\n",
        "with torch.no_grad():  # ë§¤ìš°ë§¤ìš° ì¤‘ìš”!  í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ backpropì„ í•˜ë©´ ì•ˆ ë¨.\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(47):\n",
        "    print('%5s í´ë˜ìŠ¤ì˜ ì •ë‹µë¥  : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "metadata": {
        "id": "FCMx5F-my-Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì—¬ê¸° ìˆ˜ì •!!!!!!!\n",
        "\n",
        "# ë°ì´í„° ë¡œë” ì¤€ë¹„\n",
        "classes, train_loader, test_loader = Dataloader(DATASET_PATH, batch_size, split).getloader()\n",
        "\n",
        "# ëª¨ë¸Â·ì†ì‹¤Â·ìµœì í™”ê¸° ì„¤ì •\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# í•™ìŠµ ê¸°ë¡ìš©\n",
        "training_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "# ìµœê³  ì •í™•ë„ ì¶”ì \n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    # í‰ê°€ ë‹¨ê³„\n",
        "    net.eval()\n",
        "    running_test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_test_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    avg_test_loss = running_test_loss / len(test_loader)\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "    acc = 100.0 * correct / total\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f'Epoch {epoch}/{num_epochs} â€” Train Loss: {avg_train_loss:.4f}, '\n",
        "          f'Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%')\n",
        "\n",
        "    # ìµœê³  ì •í™•ë„ ê°±ì‹  ë° ëª¨ë¸ ì €ì¥\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(net.state_dict(), 'best_model.pth')\n",
        "        print(f'â–¶ New Best Accuracy! {best_acc:.2f}% ì—í¬í¬ {epoch}ì—ì„œ ë‹¬ì„±, ëª¨ë¸ ì €ì¥ ì™„ë£Œ.')\n",
        "\n",
        "#í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
        "plt.plot(training_loss_history, label=\"Training Loss\")\n",
        "plt.plot(test_loss_history, 'r', label=\"Test Loss\")\n",
        "plt.title('Training & Test Loss', fontsize=20)\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ì €ì¥ëœ ìµœê³  ëª¨ë¸ ë¡œë“œ ë° ìƒ˜í”Œ ì˜ˆì¸¡\n",
        "net_best = Net()\n",
        "net_best.load_state_dict(torch.load('best_model.pth'))\n",
        "net_best.eval()\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "outputs = net_best(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('ì‹¤ì œ ì •ë‹µ: ', ', '.join(f'{classes[l]}' for l in labels[:4]))\n",
        "print('ëª¨ë¸ ì˜ˆì¸¡: ', ', '.join(f'{classes[p]}' for p in predicted[:4]))"
      ],
      "metadata": {
        "id": "v9GFhzdGEqIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„\n",
        "* í‰ê°€ ì§€í‘œ\n",
        "* ë¹„êµ ëª¨ë¸ (ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ ë¶„ì„)\n",
        "* graph/í‘œ êµ¬ì„±\n",
        "tip: ë²¤ì¹˜ë§ˆí¬ - ê¸°ì¡´ì— ì–´ë–¤ ì—°êµ¬ê°€ ìˆëŠ”ì§€ ì‚´í´ë³´ê³ , ê·¸ëŒ€ë¡œ ì§„í–‰í•˜ë˜ ì„±ëŠ¥ì„ ì˜¬ë¦´ ë°©ë²• ëª¨ìƒ‰í•˜ê¸° - ëª¨ë¸ í•˜ë‚˜ ì¡ì•„ì„œ ê·¸ê²Œ ì–´ë–¤ dataë¥¼ ì¼ëŠ”ì§€ ì‚´í´ë³´ê¸° (ë…¼ë¬¸) : ì½”ë“œ ê°€ì ¸ì™€ì„œ ìˆ˜ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ"
      ],
      "metadata": {
        "id": "bDFZQk_qskIE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0DYer-ehskOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë§Œì•½ì— lossë¥¼ í”„ë¦°íŠ¸í–ˆëŠ”ë° train lossëŠ” ë–¨ì–´ì§€ëŠ”ë° test lossëŠ” ì•ˆ ë–¨ì–´ì§€ëŠ” ê±°: overfitting -> data augmentationì„ í•˜ê±°ë‚˜ (generalization ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„), early stopping(epoch ì¤„ì´ê¸°), dropout layer ì¶”ê°€í•˜ê¸°\n",
        "\n",
        "train lossê°€ ì§„ë™: underfitting: a(learning rate)ê°€ ë„ˆë¬´ í° ê²½ìš° -> ì¤„ì´ê¸°, loss functionì„ ë‹¤ì‹œ ì²´í¬, network ì„¤ê³„ë¥¼ ë‹¤ì‹œ ëœ¯ì–´ë³´ê¸°\n",
        "\n",
        "í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼(lossê°€ ë‘˜ ë‹¤ ë–¨ì–´ì§€ê¸´ í•˜ëŠ”ë°): learning rate scheduler ì‚¬ìš©í•´ì„œ ì²˜ìŒì—ëŠ” aë¥¼ í¬ê²Œ í–ˆë‹¤ê°€ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ ê°ì†Œí•˜ë„ë¡ ì„¤ê³„ cosineannealinglr\n",
        "optimizerì„ ì•„ë‹´ì—ì„œ ì•„ë‹´Wë¡œ\n",
        "batch normalization ì¶”ê°€"
      ],
      "metadata": {
        "id": "J5G4PWt--GfW"
      }
    }
  ]
}