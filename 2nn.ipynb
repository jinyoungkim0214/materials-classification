{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinyoungkim0214/portfolio/blob/main/2nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCYO6dmGgefe"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er0RD438gRLm",
        "outputId": "99a00155-abbe-4ccb-9791-11340e94093b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfeql_8sgnKJ",
        "outputId": "7e1f7201-babd-42c7-a9f0-afa38f325f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Homework 1-1\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd /content/drive/MyDrive/Homework 1-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEoabX-hGCh"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyammZP8hI7P"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist.data_utils import load_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxTNOvI5NHD"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuQB6W2U5ZE2"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def softmax(X):\n",
        "    \"\"\"\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    logit = np.exp(X-np.amax(X, axis=1, keepdims=True))\n",
        "    numer = logit\n",
        "    denom = np.sum(logit, axis=1, keepdims=True)\n",
        "    return numer/denom\n",
        "\n",
        "def load_batch(X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches with the remainder dropped.\n",
        "\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        permutation = np.random.permutation(X.shape[0])\n",
        "        X = X[permutation, :]\n",
        "        Y = Y[permutation, :]\n",
        "    num_steps = int(X.shape[0])//batch_size\n",
        "    step = 0\n",
        "    while step<num_steps:\n",
        "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
        "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
        "        step+=1\n",
        "        yield X_batch, Y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsU8v_6khR30"
      },
      "source": [
        "#2-Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA5udiGmhRb5"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\" a neural network with 2 layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        Do NOT modify this function.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
        "\n",
        "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        initializes parameters with Xavier Initialization.\n",
        "\n",
        "        Question (a)\n",
        "        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization\n",
        "\n",
        "        Inputs\n",
        "        - input_dim\n",
        "        - num_hiddens: the number of hidden units\n",
        "        - num_classes: the number of classes\n",
        "        Returns\n",
        "        - params: a dictionary with the initialized parameters.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        params['W1'] = np.random.randn(input_dim, num_hiddens) * np.sqrt(2 / (input_dim + num_hiddens))\n",
        "        params['b1'] = np.zeros(num_hiddens)\n",
        "        params['W2'] = np.random.randn(num_hiddens, num_classes) * np.sqrt(2 / (num_hiddens + num_classes))\n",
        "        params['b2'] = np.zeros(num_classes)\n",
        "        return params\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Defines and performs the feed forward step of a two-layer neural network.\n",
        "        Specifically, the network structue is given by\n",
        "\n",
        "          y = softmax(sigmoid(X W1 + b1) W2 + b2)\n",
        "\n",
        "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "        Question (b)\n",
        "        - ff_dict will be used to run backpropagation in backward method.\n",
        "\n",
        "        Inputs\n",
        "        - X: the input matrix of shape (N, D)\n",
        "\n",
        "        Returns\n",
        "        - y: the output of the model\n",
        "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
        "        \"\"\"\n",
        "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
        "        y = softmax(np.dot(sigmoid(np.dot(X,W1) + b1), W2) + b2)\n",
        "        ff_dict = {'X': X, 'z1': np.dot(X,W1) + b1, 'a1': sigmoid(np.dot(X,W1) + b1), 'z2': np.dot(sigmoid(np.dot(X,W1) + b1), W2) + b2, 'y': y}\n",
        "        return y, ff_dict\n",
        "\n",
        "    def backward(self, X, Y, ff_dict):\n",
        "        \"\"\"\n",
        "        Performs backpropagation over the two-layer neural network, and returns\n",
        "        a dictionary of gradients of all model parameters.\n",
        "\n",
        "        Question (c)\n",
        "\n",
        "        Inputs:\n",
        "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "              in a mini-batch, D is the feature dimensionality.\n",
        "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "              where B is the number of examples in a mini-batch, C is the number\n",
        "              of classes.\n",
        "         - ff_dict: the dictionary containing all the fully connected units and\n",
        "              activations.\n",
        "\n",
        "        Returns:\n",
        "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
        "        \"\"\"\n",
        "        B = X.shape[0]  # Batch size\n",
        "        a1, y = ff_dict['a1'], ff_dict['y']\n",
        "\n",
        "        # Backward pass\n",
        "        grads = {}\n",
        "        grads['W2'] = np.dot(a1.T, (y - Y)) / B\n",
        "        grads['b2'] = np.sum((y - Y), axis=0) / B\n",
        "        delta2 = np.dot(y - Y, self.params['W2'].T) * a1 * (1 - a1)\n",
        "        grads['W1'] = np.dot(X.T, delta2) / B\n",
        "        grads['b1'] = np.sum(delta2, axis=0) / B\n",
        "        return grads\n",
        "\n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes cross entropy loss.\n",
        "\n",
        "        Do NOT modify this function.\n",
        "\n",
        "        Inputs\n",
        "            Y: ground truth labels\n",
        "            Y_hat: predicted labels\n",
        "        Returns\n",
        "            loss:\n",
        "        \"\"\"\n",
        "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
        "        \"\"\"\n",
        "        Runs mini-batch gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X\n",
        "        - Y\n",
        "        - X_val: validation data\n",
        "        - Y_Val: validation label\n",
        "        - lr: learning rate\n",
        "        - n_epochs: the number of epochs to run\n",
        "        - batch_size\n",
        "        - log_interval: the epoch interval to log the training progress.\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
        "                self.train_step(X_batch, Y_batch, batch_size, lr)\n",
        "            if epoch % log_interval==0:\n",
        "                Y_hat, ff_dict = self.forward(X)\n",
        "                train_loss = self.compute_loss(Y, Y_hat)\n",
        "                train_acc = self.evaluate(Y, Y_hat)\n",
        "                Y_hat, ff_dict = self.forward(X_val)\n",
        "                valid_loss = self.compute_loss(Y_val, Y_hat)\n",
        "                valid_acc = self.evaluate(Y_val, Y_hat)\n",
        "                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
        "                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
        "        \"\"\"\n",
        "        Updates the parameters using gradient descent.\n",
        "\n",
        "        Question (d)\n",
        "\n",
        "        Inputs\n",
        "        - X_batch\n",
        "        - Y_batch\n",
        "        - batch_size\n",
        "        - lr: learning rate\n",
        "        \"\"\"\n",
        "        _, ff_dict = self.forward(X_batch)\n",
        "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
        "\n",
        "        self.params['W1'] -= lr * grads['W1']\n",
        "        self.params['b1'] -= lr * grads['b1']\n",
        "        self.params['W2'] -= lr * grads['W2']\n",
        "        self.params['b2'] -= lr * grads['b2']\n",
        "\n",
        "    def evaluate(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes classification accuracy.\n",
        "\n",
        "        Question (e)\n",
        "\n",
        "        Inputs\n",
        "        - Y: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
        "             where C is the number of classes.\n",
        "        - Y_hat: A numpy array of shape (N, C) containing the softmax outputs,\n",
        "             where C is the number of classes.\n",
        "\n",
        "        Returns\n",
        "            accuracy: the classification accuracy in float\n",
        "        \"\"\"\n",
        "        predicted_labels = np.argmax(Y_hat, axis=1)\n",
        "        true_labels = np.argmax(Y, axis=1)\n",
        "        accuracy = np.mean(predicted_labels == true_labels)\n",
        "\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM2lWhtDYC6"
      },
      "source": [
        "#Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ooR6YIxYhC",
        "outputId": "f06a775b-8345-4615-809b-2f4b22e975af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNIST data loaded:\n",
            "Training data shape: (60000, 784)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "Set validation data aside\n",
            "Training data shape:  (48000, 784)\n",
            "Training labels shape:  (48000, 10)\n",
            "Validation data shape:  (12000, 784)\n",
            "Validation labels shape:  (12000, 10)\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data()\n",
        "\n",
        "idxs = np.arange(len(X_train))\n",
        "np.random.shuffle(idxs)\n",
        "split_idx = int(np.ceil(len(idxs)*0.8))\n",
        "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
        "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
        "print()\n",
        "print('Set validation data aside')\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', Y_train.shape)\n",
        "print('Validation data shape: ', X_valid.shape)\n",
        "print('Validation labels shape: ', Y_valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzw-D4Zr5xoi"
      },
      "source": [
        "#Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlnC_rerHPaN"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Question (f)\n",
        "# Tune the hyperparameters with validation data,\n",
        "# and print the results by running the lines below.\n",
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCqVT4S0Tm5"
      },
      "outputs": [],
      "source": [
        "# model instantiation\n",
        "model = TwoLayerNN(input_dim=784, num_hiddens=10, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6cWb6xg0NxOs",
        "outputId": "8ec37f78-f5a0-4d50-8136-0f7e449a06e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 00 - train loss/acc: 0.521 0.874, valid loss/acc: 0.527 0.869\n",
            "epoch 01 - train loss/acc: 0.374 0.902, valid loss/acc: 0.382 0.896\n",
            "epoch 02 - train loss/acc: 0.328 0.911, valid loss/acc: 0.339 0.906\n",
            "epoch 03 - train loss/acc: 0.305 0.915, valid loss/acc: 0.321 0.908\n",
            "epoch 04 - train loss/acc: 0.289 0.919, valid loss/acc: 0.307 0.911\n",
            "epoch 05 - train loss/acc: 0.274 0.923, valid loss/acc: 0.296 0.913\n",
            "epoch 06 - train loss/acc: 0.268 0.924, valid loss/acc: 0.291 0.915\n",
            "epoch 07 - train loss/acc: 0.258 0.926, valid loss/acc: 0.285 0.918\n",
            "epoch 08 - train loss/acc: 0.254 0.927, valid loss/acc: 0.284 0.917\n",
            "epoch 09 - train loss/acc: 0.249 0.928, valid loss/acc: 0.279 0.918\n",
            "epoch 10 - train loss/acc: 0.244 0.931, valid loss/acc: 0.276 0.919\n",
            "epoch 11 - train loss/acc: 0.241 0.931, valid loss/acc: 0.278 0.919\n",
            "epoch 12 - train loss/acc: 0.236 0.933, valid loss/acc: 0.275 0.920\n",
            "epoch 13 - train loss/acc: 0.233 0.933, valid loss/acc: 0.273 0.921\n",
            "epoch 14 - train loss/acc: 0.227 0.935, valid loss/acc: 0.268 0.922\n",
            "epoch 15 - train loss/acc: 0.224 0.936, valid loss/acc: 0.266 0.922\n",
            "epoch 16 - train loss/acc: 0.221 0.936, valid loss/acc: 0.267 0.922\n",
            "epoch 17 - train loss/acc: 0.222 0.937, valid loss/acc: 0.265 0.925\n",
            "epoch 18 - train loss/acc: 0.216 0.937, valid loss/acc: 0.263 0.922\n",
            "epoch 19 - train loss/acc: 0.221 0.936, valid loss/acc: 0.266 0.922\n",
            "epoch 20 - train loss/acc: 0.215 0.939, valid loss/acc: 0.263 0.924\n",
            "epoch 21 - train loss/acc: 0.214 0.939, valid loss/acc: 0.264 0.924\n",
            "epoch 22 - train loss/acc: 0.213 0.939, valid loss/acc: 0.264 0.922\n",
            "epoch 23 - train loss/acc: 0.206 0.941, valid loss/acc: 0.260 0.925\n",
            "epoch 24 - train loss/acc: 0.207 0.941, valid loss/acc: 0.260 0.923\n",
            "epoch 25 - train loss/acc: 0.206 0.940, valid loss/acc: 0.262 0.923\n",
            "epoch 26 - train loss/acc: 0.206 0.940, valid loss/acc: 0.260 0.925\n",
            "epoch 27 - train loss/acc: 0.201 0.942, valid loss/acc: 0.259 0.924\n",
            "epoch 28 - train loss/acc: 0.198 0.943, valid loss/acc: 0.256 0.925\n",
            "epoch 29 - train loss/acc: 0.199 0.943, valid loss/acc: 0.261 0.925\n",
            "epoch 30 - train loss/acc: 0.197 0.944, valid loss/acc: 0.259 0.924\n",
            "epoch 31 - train loss/acc: 0.196 0.944, valid loss/acc: 0.254 0.926\n",
            "epoch 32 - train loss/acc: 0.194 0.945, valid loss/acc: 0.257 0.926\n",
            "epoch 33 - train loss/acc: 0.193 0.945, valid loss/acc: 0.256 0.924\n",
            "epoch 34 - train loss/acc: 0.193 0.945, valid loss/acc: 0.255 0.925\n",
            "epoch 35 - train loss/acc: 0.191 0.945, valid loss/acc: 0.254 0.926\n",
            "epoch 36 - train loss/acc: 0.190 0.945, valid loss/acc: 0.258 0.926\n",
            "epoch 37 - train loss/acc: 0.189 0.945, valid loss/acc: 0.255 0.926\n",
            "epoch 38 - train loss/acc: 0.193 0.944, valid loss/acc: 0.258 0.924\n",
            "epoch 39 - train loss/acc: 0.187 0.946, valid loss/acc: 0.257 0.925\n",
            "epoch 40 - train loss/acc: 0.186 0.947, valid loss/acc: 0.258 0.926\n",
            "epoch 41 - train loss/acc: 0.188 0.946, valid loss/acc: 0.256 0.927\n",
            "epoch 42 - train loss/acc: 0.184 0.948, valid loss/acc: 0.257 0.925\n",
            "epoch 43 - train loss/acc: 0.186 0.947, valid loss/acc: 0.257 0.925\n",
            "epoch 44 - train loss/acc: 0.186 0.946, valid loss/acc: 0.262 0.924\n",
            "epoch 45 - train loss/acc: 0.183 0.948, valid loss/acc: 0.257 0.926\n",
            "epoch 46 - train loss/acc: 0.182 0.948, valid loss/acc: 0.255 0.925\n",
            "epoch 47 - train loss/acc: 0.188 0.945, valid loss/acc: 0.262 0.922\n",
            "epoch 48 - train loss/acc: 0.179 0.949, valid loss/acc: 0.256 0.926\n",
            "epoch 49 - train loss/acc: 0.179 0.948, valid loss/acc: 0.258 0.924\n",
            "epoch 50 - train loss/acc: 0.180 0.948, valid loss/acc: 0.260 0.925\n",
            "epoch 51 - train loss/acc: 0.182 0.946, valid loss/acc: 0.263 0.923\n",
            "epoch 52 - train loss/acc: 0.178 0.950, valid loss/acc: 0.259 0.925\n",
            "epoch 53 - train loss/acc: 0.178 0.948, valid loss/acc: 0.258 0.925\n",
            "epoch 54 - train loss/acc: 0.178 0.948, valid loss/acc: 0.257 0.926\n",
            "epoch 55 - train loss/acc: 0.172 0.951, valid loss/acc: 0.254 0.928\n",
            "epoch 56 - train loss/acc: 0.175 0.950, valid loss/acc: 0.255 0.926\n",
            "epoch 57 - train loss/acc: 0.172 0.951, valid loss/acc: 0.255 0.927\n",
            "epoch 58 - train loss/acc: 0.174 0.949, valid loss/acc: 0.258 0.925\n",
            "epoch 59 - train loss/acc: 0.174 0.949, valid loss/acc: 0.258 0.926\n",
            "epoch 60 - train loss/acc: 0.172 0.951, valid loss/acc: 0.259 0.927\n",
            "epoch 61 - train loss/acc: 0.170 0.950, valid loss/acc: 0.259 0.926\n",
            "epoch 62 - train loss/acc: 0.169 0.952, valid loss/acc: 0.255 0.926\n",
            "epoch 63 - train loss/acc: 0.174 0.949, valid loss/acc: 0.264 0.923\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e3c5c440186d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model & evaluate with validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-b5dcfcb8a50c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a97b1a479c66>\u001b[0m in \u001b[0;36mload_batch\u001b[0;34m(X, Y, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train the model & evaluate with validation data\n",
        "lr, n_epochs, batch_size = 0.1, 100, 32\n",
        "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpPsAlXU0T_Z",
        "outputId": "2e9f9d48-db11-4833-cfd6-f1b1d0ba3b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test loss = 2.356, acc = 0.114\n"
          ]
        }
      ],
      "source": [
        "# evalute the model on test data\n",
        "Y_hat, _ = model.forward(X_test)\n",
        "test_loss = model.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hh1PZpk_g0I"
      },
      "source": [
        "# Extra Credit (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R0n6y9_AgXc"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "    \"\"\"\n",
        "    initializes parameters with He Initialization.\n",
        "\n",
        "    Question (g)\n",
        "    - refer to https://paperswithcode.com/method/he-initialization for He initialization\n",
        "\n",
        "    Inputs\n",
        "    - input_dim\n",
        "    - num_hiddens\n",
        "    - num_classes\n",
        "    Returns\n",
        "    - params: a dictionary with the initialized parameters.\n",
        "    \"\"\"\n",
        "\n",
        "def forward_leaky_relu(self, X):\n",
        "    \"\"\"\n",
        "    Defines and performs the feed forward step of a two-layer neural network.\n",
        "    Specifically, the network structue is given by\n",
        "\n",
        "        y = softmax(leaky_relu(X W1 + b1) W2 + b2)\n",
        "\n",
        "    where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "    of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "    The gradient of the leaky relu activation is 1 for x > 0 and 0.01 for x <= 0.\n",
        "\n",
        "    Question (g)\n",
        "\n",
        "    Inputs\n",
        "        X: the input matrix of shape (N, D)\n",
        "\n",
        "    Returns\n",
        "        y: the output of the model\n",
        "        ff_dict: a dictionary containing all the fully connected units and activations.\n",
        "    \"\"\"\n",
        "\n",
        "def backward_leaky_relu(self, X, Y, ff_dict):\n",
        "    \"\"\"\n",
        "    Performs backpropagation over the two-layer neural network, and returns\n",
        "    a dictionary of gradients of all model parameters.\n",
        "\n",
        "    Question (g)\n",
        "\n",
        "    Inputs:\n",
        "        - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "            in a mini-batch, D is the feature dimensionality.\n",
        "        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "            where B is the number of examples in a mini-batch, C is the number\n",
        "            of classes.\n",
        "        - ff_dict: the dictionary containing all the fully connected units and\n",
        "            activations.\n",
        "\n",
        "    Returns:\n",
        "        - grads: a dictionary containing the gradients of corresponding weights\n",
        "            and biases.\n",
        "    \"\"\"\n",
        "\n",
        "TwoLayerNNLeakyRelu = copy.copy(TwoLayerNN)\n",
        "TwoLayerNNLeakyRelu.initialize_parameters = initialize_parameters\n",
        "TwoLayerNNLeakyRelu.feed_forward = forward_leaky_relu\n",
        "TwoLayerNNLeakyRelu.back_propagate = backward_leaky_relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSXnk6y9vAZn"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Question (g)\n",
        "# Tune the hyperparameters with validation data,\n",
        "# and print the results by running the lines below.\n",
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-jJRXqsBxzh"
      },
      "outputs": [],
      "source": [
        "# model instantiation\n",
        "model_leaky_relu = TwoLayerNNLeakyRelu(input_dim=0, num_hiddens=0, num_classes=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC8f80a0w53m"
      },
      "outputs": [],
      "source": [
        "# train the model & evaluate with validation data\n",
        "lr, n_epochs, batch_size = 0, 0, 0\n",
        "history = model_leaky_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i__6TfpCqOc"
      },
      "outputs": [],
      "source": [
        "Y_hat, _ = model_leaky_relu.forward(X_test)\n",
        "test_loss = model_leaky_relu.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model_leaky_relu.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}